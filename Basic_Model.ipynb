{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import timeit\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def get_csv(filename,pred_y):\n",
    "    #print(pred_y.shape)\n",
    "    #duration_content = np.rint(pred_y)\n",
    "    duration_content = np.array(pred_y)\n",
    "    #print(duration_content.shape)\n",
    "    \n",
    "    #print(duration_content)\n",
    "    id_content = np.array(test_id)\n",
    "    #print(id_content.shape)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'id':id_content,\n",
    "        'trip_duration':duration_content\n",
    "    })\n",
    "\n",
    "    # Create the csv file\n",
    "    fn = filename + '.csv'\n",
    "    df.to_csv(fn,index= False)\n",
    "    \n",
    "    \n",
    "def get_csv_cluster(filename,pred_y,testid):\n",
    "    #print(pred_y.shape)\n",
    "    duration_content = np.array(pred_y)\n",
    "    #duration_content = np.array(duration_content)\n",
    "    #print(duration_content.shape)\n",
    "    \n",
    "    #print(duration_content)\n",
    "    id_content = np.array(testid)\n",
    "    #print(id_content.shape)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'id':id_content,\n",
    "        'trip_duration':duration_content\n",
    "    })\n",
    "\n",
    "    # Create the csv file\n",
    "    fn = filename + '.csv'\n",
    "    df.to_csv(fn,index= False)\n",
    "    \n",
    "    \n",
    "def get_poly(degree,x,test):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    x_ = poly.fit_transform(x)\n",
    "    test_ = poly.fit_transform(test)\n",
    "    \n",
    "    return x_,test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting X, Y (Modify filename to get different version of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train_x = pd.read_csv('train_x.csv')   # Change this\n",
    "train_y = pd.read_csv('train_y.csv',header = -1)   # Change this\n",
    "test_x = pd.read_csv('original_test.csv')  # Change this\n",
    "test_id = test['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1458644, 11)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "0.869 1159 <br>\n",
    "fit time is: 0.5949127580024651<br>\n",
    "predict time is: 0.01478029000281822<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wayne/anaconda3/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.795174908998888\n",
      "predict time is: 0.0670001879989286\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y = regr.predict(test_x)\n",
    "pred_y = pred_y.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_basic_LR',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "0.84502 1151 <br>\n",
    "fit time is: 12.516197695003939 <br>\n",
    "predict time is: 0.01478029000281822 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 14.081076761998702\n",
      "predict time is: 0.0670001879989286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_,test_ = get_poly(2,train_x,test_x)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr.fit(x_, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "#start_time = timeit.default_timer()\n",
    "pred_y = regr.predict(test_)\n",
    "pred_y = pred_y.flatten()\n",
    "#predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_basic_PR',pred_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "0.87012 1161<br>\n",
    "fit time is: 0.6580201340038911 <br>\n",
    "predict time is: 0.01415189600083977<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.9143313949971343\n",
      "predict time is: 0.08704370600025868\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#clf = linear_model.Lasso(alpha=cv_list[max_idx])\n",
    "clf = linear_model.Lasso(alpha=1.0)\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_basic_LASSO',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "0.86982 1159\n",
    "fit time is: 0.3165850510049495 <br>\n",
    "predict time is: 0.010517886999878101 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.3443432260028203\n",
      "predict time is: 0.0518609739956446\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = Ridge(alpha=1.0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_basic_Ridge',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "0.88073  <br>\n",
    "fit time is: 38.12283402299363 <br>\n",
    "predict time is: 1.2900892430043314 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 50.027936015001615\n",
      "predict time is: 0.8488182869987213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = MLPRegressor(hidden_layer_sizes=(100,),  activation='relu', solver='adam',    alpha=0.001,batch_size='auto',\n",
    "               learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "               random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n",
    "               nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "               epsilon=1e-08)\n",
    "\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_basic_NN',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression\n",
    "0.61829 <br>\n",
    "fit time is: 24.759334297996247 <br>\n",
    "predict time is: 0.5807069809961831 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 27.161054226999113\n",
      "predict time is: 0.580583688999468\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = DecisionTreeRegressor()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_basic_DTR',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN\n",
    "0.8891\n",
    "fit time is: 0.673529542000324<BR>\n",
    "predict time is: 0.0554104409966385<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.5687467649986502\n",
      "predict time is: 0.03425443600281142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = ElasticNet(random_state=0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_basic_ENet',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Regression\n",
    "0.82194<BR>\n",
    "fit time is: 14.449963496997952<BR>\n",
    "predict time is: 0.2338456350043998<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 14.352549649003777\n",
      "predict time is: 0.14448166600050172\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_basic_RFR',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 127.87939127400023\n",
      "predict time is: 1.2162127530027647\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = XGBRegressor()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_basic_XGB',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best k for the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters=2, The Silhouette Coefficient with a size of 10000 is 0.6064980968301614 \n",
      "For n_clusters=3, The Silhouette Coefficient with a size of 10000 is 0.5850858745524484 \n",
      "For n_clusters=4, The Silhouette Coefficient with a size of 10000 is 0.5849592261445825 \n",
      "For n_clusters=5, The Silhouette Coefficient with a size of 10000 is 0.576530512949516 \n",
      "For n_clusters=6, The Silhouette Coefficient with a size of 10000 is 0.5686804948610907 \n"
     ]
    }
   ],
   "source": [
    "num_list = [2,3,4,5,6]\n",
    "size_list = [10000]\n",
    "results = []\n",
    "for n_cluster in num_list:\n",
    "    kmeans = KMeans(n_clusters=n_cluster).fit(train_x)\n",
    "    label = kmeans.fit_predict(train_x)\n",
    "    for size in size_list:\n",
    "        sil_coeff = silhouette_score(train_x[:size], label[:size], metric='euclidean')\n",
    "        results.append(sil_coeff)\n",
    "        #store.append(sil_coeff)\n",
    "        print(\"For n_clusters={}, The Silhouette Coefficient with a size of {} is {} \".format(n_cluster, size,sil_coeff))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training set based on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "best_k = num_list[np.argmax(np.array(results))]\n",
    "print(best_k)\n",
    "\n",
    "kmeans = KMeans(n_clusters=best_k).fit(train_x)\n",
    "labels = kmeans.predict(train_x)\n",
    "#print(labels)\n",
    "\n",
    "train_x_cluster = train_x.copy()\n",
    "\n",
    "train_x_cluster['cluster'] = labels\n",
    "train_x_cluster['duration'] = train_y\n",
    "\n",
    "mask = train_x_cluster['cluster'] ==0\n",
    "train_x_c0 = train_x_cluster[mask]\n",
    "train_x_c1 = train_x_cluster[~mask]\n",
    "\n",
    "\n",
    "train_y_c0 = train_x_c0['duration']\n",
    "train_y_c1 = train_x_c1['duration']\n",
    "train_x_c0 = train_x_c0.drop(['duration'],axis=1)\n",
    "train_x_c1 = train_x_c1.drop(['duration'],axis=1)\n",
    "\n",
    "predict_labels = kmeans.predict(test_x)\n",
    "test_x_temp = test_x.copy()\n",
    "test_x_temp['cluster'] = predict_labels\n",
    "test_x_temp['testid'] = test_id\n",
    "\n",
    "\n",
    "\n",
    "mask = test_x_temp['cluster'] ==0\n",
    "test_x_c0 = test_x_temp[mask]\n",
    "test_x_c1 = test_x_temp[~mask]\n",
    "\n",
    "test_id_c0 = test_x_c0['testid']\n",
    "test_x_c0 = test_x_c0.drop(['testid'],axis=1)\n",
    "test_id_c1 = test_x_c1['testid']\n",
    "test_x_c1 = test_x_c1.drop(['testid'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Train Linear\n",
    "From 0.869  to 0.86137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.570997365997755\n",
      "predict time is: 0.04641183000057936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = linear_model.LinearRegression()\n",
    "regr2 = linear_model.LinearRegression()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_basic_LR_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Train PR\n",
    "From 0.84502 to 0.81918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 13.25679842200043\n",
      "predict time is: 0.07034314399788855\n"
     ]
    }
   ],
   "source": [
    "x_,test_ = get_poly(2,train_x_c0,test_x_c0)\n",
    "x2_,test2_ = get_poly(2,train_x_c1,test_x_c1)\n",
    "\n",
    "\n",
    "regr1 = linear_model.LinearRegression()\n",
    "regr2 = linear_model.LinearRegression()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(x_, train_y_c0)\n",
    "regr2.fit(x2_, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test2_)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_basic_PR_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReTrain Lasso\n",
    "0.87012 to 0.86344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.8398794469976565\n",
      "predict time is: 0.04548414199962281\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = linear_model.Lasso(alpha=1.0)\n",
    "regr2 = linear_model.Lasso(alpha=1.0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_basic_LASSO_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReTrain Ridge\n",
    "From 0.86982 to 0.86138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.28801481599657563\n",
      "predict time is: 0.04853643199749058\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = Ridge(alpha=1.0)\n",
    "regr2 = Ridge(alpha=1.0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_basic_Ridge_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT NN\n",
    "From 0.88073 to 0.87623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 25.781755766998685\n",
      "predict time is: 0.7855055170002743\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 =  MLPRegressor(hidden_layer_sizes=(100,),  activation='relu', solver='adam',    alpha=0.001,batch_size='auto',\n",
    "               learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "               random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n",
    "               nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "               epsilon=1e-08)\n",
    "regr2 =  MLPRegressor(hidden_layer_sizes=(100,),  activation='relu', solver='adam',    alpha=0.001,batch_size='auto',\n",
    "               learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "               random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n",
    "               nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "               epsilon=1e-08)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_basic_NN_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT DTR\n",
    "From 0.61829 to 0.62575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 25.29299542199442\n",
      "predict time is: 0.469332092005061\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = DecisionTreeRegressor()\n",
    "regr2 = DecisionTreeRegressor()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_basic_DTR_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT EN\n",
    "0.88832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.5232038100002683\n",
      "predict time is: 0.05048355399776483\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = ElasticNet(random_state=0)\n",
    "regr2 = ElasticNet(random_state=0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_basic_ENet_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT RF\n",
    "0.82167<BR>\n",
    "fit time is: 14.623066553998797 <BR>\n",
    "predict time is: 0.13730818399926648<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 14.191378748997522\n",
      "predict time is: 0.1408168099951581\n"
     ]
    }
   ],
   "source": [
    "regr1 = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr2 = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_basic_RF_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 129.2343672930001\n",
      "predict time is: 1.7947825160008506\n"
     ]
    }
   ],
   "source": [
    "regr1 = XGBRegressor()\n",
    "regr2 = XGBRegressor()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_basic_XGB_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
