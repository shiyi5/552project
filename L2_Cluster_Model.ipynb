{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import timeit\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def get_csv(filename,pred_y):\n",
    "    #print(pred_y.shape)\n",
    "    duration_content = np.rint(pred_y)\n",
    "    #duration_content = np.array(duration_content)\n",
    "    #print(duration_content.shape)\n",
    "    \n",
    "    #print(duration_content)\n",
    "    id_content = np.array(test_id)\n",
    "    #print(id_content.shape)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'id':id_content,\n",
    "        'trip_duration':duration_content\n",
    "    })\n",
    "\n",
    "    # Create the csv file\n",
    "    fn = filename + '.csv'\n",
    "    df.to_csv(fn,index= False)\n",
    "    \n",
    "    \n",
    "def get_csv_cluster(filename,pred_y,testid):\n",
    "    #print(pred_y.shape)\n",
    "    duration_content = np.rint(pred_y)\n",
    "    #duration_content = np.array(duration_content)\n",
    "    #print(duration_content.shape)\n",
    "    \n",
    "    #print(duration_content)\n",
    "    id_content = np.array(testid)\n",
    "    #print(id_content.shape)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'id':id_content,\n",
    "        'trip_duration':duration_content\n",
    "    })\n",
    "\n",
    "    # Create the csv file\n",
    "    fn = filename + '.csv'\n",
    "    df.to_csv(fn,index= False)\n",
    "    \n",
    "    \n",
    "def get_poly(degree,x,test):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    x_ = poly.fit_transform(x)\n",
    "    test_ = poly.fit_transform(test)\n",
    "    \n",
    "    return x_,test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting X, Y (Modify filename to get different version of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train_x = pd.read_csv('train_position_time_x.csv')   # Change this\n",
    "train_y = pd.read_csv('train_y.csv',header = -1)   # Change this\n",
    "test_x = pd.read_csv('test_position_time_x.csv')  # Change this\n",
    "test_id = test['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_x.columns.tolist()\n",
    "#print(cols)\n",
    "cols = cols[:-3] + [cols[-3]] + [cols[-2]] + [cols[-1]]\n",
    "cols2 = test_x.columns.tolist()\n",
    "cols2 = cols2[:-3]+ [cols[-3]] + [cols[-2]] + [cols[-1]]\n",
    "#print(cols)\n",
    "#print(cols2)\n",
    "test_x = test_x[cols2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "0.869 1159 <br>\n",
    "fit time is: 0.5949127580024651<br>\n",
    "predict time is: 0.01478029000281822<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wayne/anaconda3/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.9270750160212629\n",
      "predict time is: 0.06134528198163025\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y = regr.predict(test_x)\n",
    "pred_y = pred_y.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_Cluster_L2_LR',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "0.84502 1151 <br>\n",
    "fit time is: 12.516197695003939 <br>\n",
    "predict time is: 0.01478029000281822 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 25.90854251399287\n",
      "predict time is: 0.06134528198163025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_,test_ = get_poly(2,train_x,test_x)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr.fit(x_, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "#start_time = timeit.default_timer()\n",
    "pred_y = regr.predict(test_)\n",
    "pred_y = pred_y.flatten()\n",
    "#predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_Cluster_L2_PR',pred_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "0.87012 1161<br>\n",
    "fit time is: 0.6580201340038911 <br>\n",
    "predict time is: 0.01415189600083977<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 1.251267329993425\n",
      "predict time is: 0.1255442350229714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#clf = linear_model.Lasso(alpha=cv_list[max_idx])\n",
    "clf = linear_model.Lasso(alpha=1.0)\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_Cluster_L2_LASSO',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge\n",
    "0.86982 1159\n",
    "fit time is: 0.3165850510049495 <br>\n",
    "predict time is: 0.010517886999878101 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.40508901298744604\n",
      "predict time is: 0.046549402992241085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = Ridge(alpha=1.0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_Cluster_L2_Ridge',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "0.88073  <br>\n",
    "fit time is: 38.12283402299363 <br>\n",
    "predict time is: 1.2900892430043314 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 71.3068413049914\n",
      "predict time is: 1.7072043730004225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = MLPRegressor(hidden_layer_sizes=(100,),  activation='relu', solver='adam',    alpha=0.001,batch_size='auto',\n",
    "               learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "               random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n",
    "               nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "               epsilon=1e-08)\n",
    "\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_Cluster_L2_NN',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression\n",
    "0.61829 <br>\n",
    "fit time is: 24.759334297996247 <br>\n",
    "predict time is: 0.5807069809961831 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 32.73409995701513\n",
      "predict time is: 0.6296945349895395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = DecisionTreeRegressor()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_Cluster_L2_DTR',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EN\n",
    "0.8891\n",
    "fit time is: 0.673529542000324<BR>\n",
    "predict time is: 0.0554104409966385<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.796649502008222\n",
      "predict time is: 0.04481834900798276\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = ElasticNet(random_state=0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_Cluster_L2_ENet',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Regression\n",
    "0.82194<BR>\n",
    "fit time is: 14.449963496997952<BR>\n",
    "predict time is: 0.2338456350043998<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 20.53770640899893\n",
      "predict time is: 0.18060777001664974\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_Cluster_L2_RFR',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 164.43829177998123\n",
      "predict time is: 1.4962019119993784\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = XGBRegressor()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "train_y= np.array(train_y).flatten()\n",
    "clf.fit(train_x, train_y)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "pred_y = clf.predict(test_x)\n",
    "#pred_y = pred_y.flatten()\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv('SUB_Cluster_L2_XGB',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best k for the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters=2, The Silhouette Coefficient with a size of 10000 is 0.6064853717321517 \n",
      "For n_clusters=3, The Silhouette Coefficient with a size of 10000 is 0.5845275457455607 \n",
      "For n_clusters=4, The Silhouette Coefficient with a size of 10000 is 0.5847459173038736 \n",
      "For n_clusters=5, The Silhouette Coefficient with a size of 10000 is 0.5764971295314418 \n",
      "For n_clusters=6, The Silhouette Coefficient with a size of 10000 is 0.5686752217292627 \n"
     ]
    }
   ],
   "source": [
    "num_list = [2,3,4,5,6]\n",
    "size_list = [10000]\n",
    "results = []\n",
    "for n_cluster in num_list:\n",
    "    kmeans = KMeans(n_clusters=n_cluster).fit(train_x)\n",
    "    label = kmeans.fit_predict(train_x)\n",
    "    for size in size_list:\n",
    "        sil_coeff = silhouette_score(train_x[:size], label[:size], metric='euclidean')\n",
    "        results.append(sil_coeff)\n",
    "        #store.append(sil_coeff)\n",
    "        print(\"For n_clusters={}, The Silhouette Coefficient with a size of {} is {} \".format(n_cluster, size,sil_coeff))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training set based on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "best_k = num_list[np.argmax(np.array(results))]\n",
    "print(best_k)\n",
    "\n",
    "kmeans = KMeans(n_clusters=best_k).fit(train_x)\n",
    "labels = kmeans.predict(train_x)\n",
    "#print(labels)\n",
    "\n",
    "train_x_cluster = train_x.copy()\n",
    "\n",
    "train_x_cluster['cluster'] = labels\n",
    "train_x_cluster['duration'] = train_y\n",
    "\n",
    "mask = train_x_cluster['cluster'] ==0\n",
    "train_x_c0 = train_x_cluster[mask]\n",
    "train_x_c1 = train_x_cluster[~mask]\n",
    "\n",
    "\n",
    "train_y_c0 = train_x_c0['duration']\n",
    "train_y_c1 = train_x_c1['duration']\n",
    "train_x_c0 = train_x_c0.drop(['duration'],axis=1)\n",
    "train_x_c1 = train_x_c1.drop(['duration'],axis=1)\n",
    "\n",
    "predict_labels = kmeans.predict(test_x)\n",
    "test_x_temp = test_x.copy()\n",
    "test_x_temp['cluster'] = predict_labels\n",
    "test_x_temp['testid'] = test_id\n",
    "\n",
    "\n",
    "\n",
    "mask = test_x_temp['cluster'] ==0\n",
    "test_x_c0 = test_x_temp[mask]\n",
    "test_x_c1 = test_x_temp[~mask]\n",
    "\n",
    "test_id_c0 = test_x_c0['testid']\n",
    "test_x_c0 = test_x_c0.drop(['testid'],axis=1)\n",
    "test_id_c1 = test_x_c1['testid']\n",
    "test_x_c1 = test_x_c1.drop(['testid'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Train Linear\n",
    "From 0.869  to 0.86137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.7687215069890954\n",
      "predict time is: 0.06312920901109464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = linear_model.LinearRegression()\n",
    "regr2 = linear_model.LinearRegression()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_Cluster_L2_LR_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Train PR\n",
    "From 0.84502 to 0.81918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 24.90510116997757\n",
      "predict time is: 0.44824855402112007\n"
     ]
    }
   ],
   "source": [
    "x_,test_ = get_poly(2,train_x_c0,test_x_c0)\n",
    "x2_,test2_ = get_poly(2,train_x_c1,test_x_c1)\n",
    "\n",
    "\n",
    "regr1 = linear_model.LinearRegression()\n",
    "regr2 = linear_model.LinearRegression()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(x_, train_y_c0)\n",
    "regr2.fit(x2_, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test2_)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_Cluster_L2_PR_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReTrain Lasso\n",
    "0.87012 to 0.86344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 1.0466254510101862\n",
      "predict time is: 0.05336100800195709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = linear_model.Lasso(alpha=1.0)\n",
    "regr2 = linear_model.Lasso(alpha=1.0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_Cluster_L2_LASSO_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReTrain Ridge\n",
    "From 0.86982 to 0.86138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.35864824400050566\n",
      "predict time is: 0.04516135901212692\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = Ridge(alpha=1.0)\n",
    "regr2 = Ridge(alpha=1.0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_Cluster_L2_Ridge_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT NN\n",
    "From 0.88073 to 0.87623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 25.307777851005085\n",
      "predict time is: 0.6280858670070302\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 =  MLPRegressor(hidden_layer_sizes=(100,),  activation='relu', solver='adam',    alpha=0.001,batch_size='auto',\n",
    "               learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "               random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n",
    "               nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "               epsilon=1e-08)\n",
    "regr2 =  MLPRegressor(hidden_layer_sizes=(100,),  activation='relu', solver='adam',    alpha=0.001,batch_size='auto',\n",
    "               learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "               random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n",
    "               nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "               epsilon=1e-08)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_Cluster_L2_NN_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT DTR\n",
    "From 0.61829 to 0.62575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 29.746244022011524\n",
      "predict time is: 0.5898438580043148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = DecisionTreeRegressor()\n",
    "regr2 = DecisionTreeRegressor()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_Cluster_L2_DTR_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT EN\n",
    "0.88832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 0.7077525659988169\n",
      "predict time is: 0.04597004200331867\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regr1 = ElasticNet(random_state=0)\n",
    "regr2 = ElasticNet(random_state=0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_Cluster_L2_ENet_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT RF\n",
    "0.82167<BR>\n",
    "fit time is: 14.623066553998797 <BR>\n",
    "predict time is: 0.13730818399926648<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 18.832837394002127\n",
      "predict time is: 0.17414062700117938\n"
     ]
    }
   ],
   "source": [
    "regr1 = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr2 = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_Cluster_L2_RF_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RT XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit time is: 125.50903143899632\n",
      "predict time is: 1.3330116269935388\n"
     ]
    }
   ],
   "source": [
    "regr1 = XGBRegressor()\n",
    "regr2 = XGBRegressor()\n",
    "\n",
    "# Actual training\n",
    "start_time = timeit.default_timer()\n",
    "regr1.fit(train_x_c0, train_y_c0)\n",
    "regr2.fit(train_x_c1, train_y_c1)\n",
    "fit_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pred_y1 = regr1.predict(test_x_c0)\n",
    "pred_y1 = pred_y1.flatten()\n",
    "\n",
    "pred_y2 = regr2.predict(test_x_c1)\n",
    "pred_y2 = pred_y2.flatten()\n",
    "\n",
    "predict_elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "pred = np.append(pred_y1,pred_y2)\n",
    "testid = np.append(test_id_c0,test_id_c1)\n",
    "\n",
    "print (\"fit time is: {}\".format(fit_elapsed))\n",
    "print (\"predict time is: {}\".format(predict_elapsed))\n",
    "\n",
    "\n",
    "get_csv_cluster('SUB_Cluster_L2_XGB_Cluster',pred,testid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
